{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6659d6-a9da-49bf-aea6-a6fd3e341c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ac6db8-2fcb-422a-a24d-59b14691d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990fbc59-5ae9-4b83-af49-48776d2e3cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f41fd2-3256-4833-a504-dd680c4c13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating doc for text analysis\n",
    "text= \"Hello everyone! I am Prathamesh Jadhav. I am in TY pursuing Bachelors Degree in Computer Science at GSM  College Of Engineering. I have good knowledge about C, C++, Java, Jython, HTML, CSS, JS, Bootstrap and Databases. In my free time, I like listening t music as it makes me feel relaxed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f758d94-22e3-435b-8038-a1f706bc1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone!', 'I am Prathamesh Jadhav.', 'I am in TY pursuing Bachelors Degree in Computer Science at GSM  College Of Engineering.', 'I have good knowledge about C, C++, Java, Jython, HTML, CSS, JS, Bootstrap and Databases.', 'In my free time, I like listening t music as it makes me feel relaxed.']\n"
     ]
    }
   ],
   "source": [
    "#sent tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "var1 = sent_tokenize(text)\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b73e852-61c2-452f-ba2f-b7bd3c7077ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '!', 'I', 'am', 'Prathamesh', 'Jadhav', '.', 'I', 'am', 'in', 'TY', 'pursuing', 'Bachelors', 'Degree', 'in', 'Computer', 'Science', 'at', 'GSM', 'College', 'Of', 'Engineering', '.', 'I', 'have', 'good', 'knowledge', 'about', 'C', ',', 'C++', ',', 'Java', ',', 'Jython', ',', 'HTML', ',', 'CSS', ',', 'JS', ',', 'Bootstrap', 'and', 'Databases', '.', 'In', 'my', 'free', 'time', ',', 'I', 'like', 'listening', 't', 'music', 'as', 'it', 'makes', 'me', 'feel', 'relaxed', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "var2 = word_tokenize(text)\n",
    "print(var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d50fb9-eb49-488a-a602-0d0a2a99192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('!', '.'), ('I', 'PRP'), ('am', 'VBP'), ('Tulaja', 'RB'), ('Patil', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('in', 'IN'), ('TY', 'NNP'), ('pursuing', 'VBG'), ('Bachelors', 'NNS'), ('Degree', 'NNP'), ('in', 'IN'), ('Computer', 'NNP'), ('Science', 'NNP'), ('at', 'IN'), ('GSM', 'NNP'), ('College', 'NNP'), ('Of', 'IN'), ('Engineering', 'NNP'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('good', 'JJ'), ('knowledge', 'NN'), ('about', 'IN'), ('C', 'NNP'), (',', ','), ('C++', 'NNP'), (',', ','), ('Java', 'NNP'), (',', ','), ('Jython', 'NNP'), (',', ','), ('HTML', 'NNP'), (',', ','), ('CSS', 'NNP'), (',', ','), ('JS', 'NNP'), (',', ','), ('Bootstrap', 'NNP'), ('and', 'CC'), ('Databases', 'NNP'), ('.', '.'), ('In', 'IN'), ('my', 'PRP$'), ('free', 'JJ'), ('time', 'NN'), (',', ','), ('I', 'PRP'), ('like', 'VBP'), ('listening', 'VBG'), ('t', 'NN'), ('music', 'NN'), ('as', 'IN'), ('it', 'PRP'), ('makes', 'VBZ'), ('me', 'PRP'), ('feel', 'VB'), ('relaxed', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_tags = nltk.pos_tag(var2)  #pos tagging\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711bf518-198f-4ca9-893e-00da20f24442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', 'aren', \"mightn't\", 'to', 'down', 'through', 'me', 'up', 'once', 'him', 'having', 'what', \"we'd\", 'you', 'our', 'both', 'yours', 'their', 'such', 'those', 'but', 'being', 'mightn', 'no', 'myself', \"don't\", \"hasn't\", \"she's\", 'hasn', 'here', 'should', 'just', \"they've\", 'when', 'your', 'she', \"she'd\", \"that'll\", \"we've\", \"won't\", \"he'd\", 'shan', 'that', 'about', 'can', \"you'll\", 'ain', \"he'll\", \"shouldn't\", 'y', 'm', 'yourselves', 'while', 'after', 'am', 'ma', 'over', 'will', 'or', 'didn', \"mustn't\", 'himself', 'own', 'ourselves', 'before', 'too', 'mustn', 'itself', 'haven', 'some', \"they're\", 'only', 'by', 'couldn', 'very', \"haven't\", 'now', 'during', 'ours', 'all', 'if', 'my', 'has', \"isn't\", 'been', 'out', \"shan't\", 'doing', \"should've\", 'isn', 'doesn', 'have', \"it'd\", 'further', \"i've\", 'hadn', 'more', 'who', \"aren't\", 'against', 'needn', 'nor', \"she'll\", 'than', 'because', 'then', 'again', 'in', 'until', 'they', 'weren', 'where', 'd', 'he', 'll', \"he's\", 'why', 's', 'for', \"doesn't\", 'had', 'does', \"wouldn't\", 'his', 'on', 'not', 'an', 'off', \"we'll\", 'most', \"you'd\", \"they'll\", \"i'd\", \"weren't\", 'at', \"you've\", 'the', 'it', 'which', 'as', 'few', \"wasn't\", 'won', 'don', \"needn't\", 'yourself', 'we', 'are', \"it'll\", \"you're\", 'whom', 'herself', 'a', 'below', 'wouldn', 'its', 'other', 'o', 'her', 'so', 'under', \"we're\", 'between', 'be', 'shouldn', 'there', 'wasn', 'with', 'do', 'and', 'each', \"didn't\", 'were', 're', 'them', 'this', 'above', 'did', 'theirs', 'into', \"couldn't\", 've', 'from', \"i'm\", 'is', 'these', \"i'll\", \"it's\", 'same', 'of', \"hadn't\", 'themselves', 'any', 'was', \"they'd\", 't', 'hers', 'how'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ecefd9a-a293-4791-9109-5a1e3022af16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m text\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtext\u001b[49m)   \u001b[38;5;66;03m#removing punctuations\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=re.sub('[^a-zA-Z]', ' ', text)   #removing punctuations\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d3bf64-8ba1-4677-bf0b-79d4129b45f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 3\u001b[0m tokens\u001b[38;5;241m=\u001b[39mword_tokenize(\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mlower())  \u001b[38;5;66;03m#word tokenization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m filtered_txt \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens=word_tokenize(text.lower())  #word tokenization\n",
    "filtered_txt = []\n",
    "for word in tokens:\n",
    "    if word not in stopwords:\n",
    "        filtered_txt.append(word)    #removing stopwords\n",
    "print('tokenized og sentence: ',tokens)\n",
    "print('filtered sentence: ',filtered_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a6a679-6e0e-4467-8a1b-30c0eb0566f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmatization for invoking is invok\n",
      "Stemmatization for invoked is invok\n",
      "Stemmatization for reading is read\n",
      "Stemmatization for reads is read\n",
      "Stemmatization for writing is write\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#stemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "var = ['invoking', 'invoked', 'reading' , 'reads', 'writing']\n",
    "ps=PorterStemmer()\n",
    "for w in var:\n",
    "    print(\"Stemmatization for {} is {}\".format(w,ps.stem(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5991694e-3b87-4b66-bf6a-8337c6fb5ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization for invoking is invoke\n",
      "Lemmatization for invoked is invoke\n",
      "Lemmatization for reading is read\n",
      "Lemmatization for reads is read\n",
      "Lemmatization for writing is write\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl= WordNetLemmatizer()\n",
    "var = ['invoking', 'invoked', 'reading' , 'reads', 'writing']\n",
    "for w in var:\n",
    "    print(\"Lemmatization for {} is {}\".format(w,wl.lemmatize(w, pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a345da4-f3dc-4206-b1db-0047d2cf27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\program files\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\program files\\python312\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\program files\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\program files\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\program files\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66690b42-0c17-4793-bd67-24c7e7ed4fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Tulaja', 'Patil.', 'I', 'am', 'pursuing', 'Bachelors', 'of', 'Computer', 'Engineering', 'in', 'GSM', 'College', 'of', 'Engineering.', 'It', 'is', 'one', 'of', 'the', 'best', 'colleges', 'in', 'Pune,', 'falling', 'tenth', 'in', 'the', 'list.', 'I', 'have', 'good', 'knowledge', 'about', 'C,', 'C++,', 'Python,', 'Java,', 'Web', 'Development', 'and', 'Data', 'Analytics.', 'In', 'my', 'free', 'time,', 'I', 'like', 'listening', 'to', 'music', 'as', 'it', 'makes', 'me', 'relaxed.']\n",
      "['Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers.', 'A', 'peck', 'of', 'pickled', 'peppers', 'Peter', 'Piper', 'picked.', 'If', 'Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers,', 'Where’s', 'the', 'peck', 'of', 'pickled', 'peppers', 'Peter', 'Piper', 'picked?']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  #remember\n",
    " \n",
    "docA= 'I am Tulaja Patil. I am pursuing Bachelors of Computer Engineering in GSM College of Engineering. It is one of the best colleges in Pune, falling tenth in the list. I have good knowledge about C, C++, Python, Java, Web Development and Data Analytics. In my free time, I like listening to music as it makes me relaxed.'\n",
    "docB= 'Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked. If Peter Piper picked a peck of pickled peppers, Where’s the peck of pickled peppers Peter Piper picked?'\n",
    "\n",
    "bow1 = docA.split(' ')\n",
    "bow2 = docB.split(' ')\n",
    "\n",
    "print(bow1)\n",
    "print(bow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c5ef56-81b7-4847-a840-ff15222ade89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'music', 'Engineering', 'peppers,', 'C++,', 'Data', 'to', 'pursuing', 'am', 'A', 'in', 'me', 'a', 'peck', 'knowledge', 'best', 'Development', 'tenth', 'GSM', 'Piper', 'about', 'Web', 'Patil.', 'one', 'Pune,', 'Bachelors', 'College', 'and', 'picked.', 'peppers.', 'Engineering.', 'Analytics.', 'Tulaja', 'pickled', 'free', 'Java,', 'Computer', 'Peter', 'Python,', 'like', 'picked', 'If', 'I', 'is', 'falling', 'C,', 'my', 'the', 'In', 'It', 'list.', 'of', 'it', 'as', 'picked?', 'time,', 'peppers', 'colleges', 'listening', 'relaxed.', 'have', 'Where’s', 'good', 'makes'}\n"
     ]
    }
   ],
   "source": [
    "#unique words\n",
    "uw = set(bow1).union(set(bow2))\n",
    "print(uw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a903dd6-9d2a-4368-97ad-9bb37c969d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary (now1) to store the word counts for doc1\n",
    "# `dict.fromkeys(bow1, 0)` initializes all words in bow1 with a count of 0\n",
    "now1 = dict.fromkeys(bow1, 0)\n",
    "for word in bow1:\n",
    "    now1[word] +=1\n",
    "\n",
    "now2 = dict.fromkeys(bow2, 0)\n",
    "for word in bow2:\n",
    "    now2[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e518af-95bc-40b8-9c0a-fe888f60f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0.06896551724137931, 'am': 0.034482758620689655, 'Tulaja': 0.017241379310344827, 'Patil.': 0.017241379310344827, 'pursuing': 0.017241379310344827, 'Bachelors': 0.017241379310344827, 'of': 0.05172413793103448, 'Computer': 0.017241379310344827, 'Engineering': 0.017241379310344827, 'in': 0.05172413793103448, 'GSM': 0.017241379310344827, 'College': 0.017241379310344827, 'Engineering.': 0.017241379310344827, 'It': 0.017241379310344827, 'is': 0.017241379310344827, 'one': 0.017241379310344827, 'the': 0.034482758620689655, 'best': 0.017241379310344827, 'colleges': 0.017241379310344827, 'Pune,': 0.017241379310344827, 'falling': 0.017241379310344827, 'tenth': 0.017241379310344827, 'list.': 0.017241379310344827, 'have': 0.017241379310344827, 'good': 0.017241379310344827, 'knowledge': 0.017241379310344827, 'about': 0.017241379310344827, 'C,': 0.017241379310344827, 'C++,': 0.017241379310344827, 'Python,': 0.017241379310344827, 'Java,': 0.017241379310344827, 'Web': 0.017241379310344827, 'Development': 0.017241379310344827, 'and': 0.017241379310344827, 'Data': 0.017241379310344827, 'Analytics.': 0.017241379310344827, 'In': 0.017241379310344827, 'my': 0.017241379310344827, 'free': 0.017241379310344827, 'time,': 0.017241379310344827, 'like': 0.017241379310344827, 'listening': 0.017241379310344827, 'to': 0.017241379310344827, 'music': 0.017241379310344827, 'as': 0.017241379310344827, 'it': 0.017241379310344827, 'makes': 0.017241379310344827, 'me': 0.017241379310344827, 'relaxed.': 0.017241379310344827}\n",
      "{'Peter': 0.11764705882352941, 'Piper': 0.11764705882352941, 'picked': 0.058823529411764705, 'a': 0.058823529411764705, 'peck': 0.11764705882352941, 'of': 0.11764705882352941, 'pickled': 0.11764705882352941, 'peppers.': 0.029411764705882353, 'A': 0.029411764705882353, 'peppers': 0.058823529411764705, 'picked.': 0.029411764705882353, 'If': 0.029411764705882353, 'peppers,': 0.029411764705882353, 'Where’s': 0.029411764705882353, 'the': 0.029411764705882353, 'picked?': 0.029411764705882353}\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wdict, bow):\n",
    "    tfdict={}\n",
    "    now = len(bow)\n",
    "\n",
    "    for word, count in wdict.items():\n",
    "        tfdict[word] = count/float(now)\n",
    "    return tfdict\n",
    "\n",
    "tf1 = computeTF(now1, bow1)\n",
    "tf2 = computeTF(now2, bow2)\n",
    "\n",
    "print(tf1)\n",
    "print(tf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b74bb1a5-ea86-4e4b-8350-05c36d19cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'music': 0.6931471805599453, 'Engineering': 0.6931471805599453, 'C++,': 0.6931471805599453, 'Data': 0.6931471805599453, 'to': 0.6931471805599453, 'pursuing': 0.6931471805599453, 'am': 0.6931471805599453, 'in': 0.6931471805599453, 'me': 0.6931471805599453, 'knowledge': 0.6931471805599453, 'best': 0.6931471805599453, 'Development': 0.6931471805599453, 'tenth': 0.6931471805599453, 'GSM': 0.6931471805599453, 'about': 0.6931471805599453, 'Web': 0.6931471805599453, 'Patil.': 0.6931471805599453, 'one': 0.6931471805599453, 'Pune,': 0.6931471805599453, 'Bachelors': 0.6931471805599453, 'College': 0.6931471805599453, 'and': 0.6931471805599453, 'Engineering.': 0.6931471805599453, 'Analytics.': 0.6931471805599453, 'Tulaja': 0.6931471805599453, 'free': 0.6931471805599453, 'Java,': 0.6931471805599453, 'Computer': 0.6931471805599453, 'Python,': 0.6931471805599453, 'like': 0.6931471805599453, 'I': 0.6931471805599453, 'is': 0.6931471805599453, 'falling': 0.6931471805599453, 'C,': 0.6931471805599453, 'my': 0.6931471805599453, 'the': 0.0, 'In': 0.6931471805599453, 'It': 0.6931471805599453, 'list.': 0.6931471805599453, 'of': 0.0, 'it': 0.6931471805599453, 'as': 0.6931471805599453, 'time,': 0.6931471805599453, 'colleges': 0.6931471805599453, 'listening': 0.6931471805599453, 'relaxed.': 0.6931471805599453, 'have': 0.6931471805599453, 'good': 0.6931471805599453, 'makes': 0.6931471805599453, 'picked.': 0.6931471805599453, 'peppers.': 0.6931471805599453, 'peppers,': 0.6931471805599453, 'pickled': 0.6931471805599453, 'Piper': 0.6931471805599453, 'Peter': 0.6931471805599453, 'A': 0.6931471805599453, 'picked': 0.6931471805599453, 'peppers': 0.6931471805599453, 'If': 0.6931471805599453, 'picked?': 0.6931471805599453, 'a': 0.6931471805599453, 'peck': 0.6931471805599453, 'Where’s': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(docs):\n",
    "    N = len(docs)\n",
    "    doc_freq = {}  # Create a dictionary to store the document frequency of each word\n",
    "\n",
    "    for doc in docs:\n",
    "        uniquew = set(doc.split())# Only consider unique words in the document\n",
    "        for word in uniquew:\n",
    "            if word not in doc_freq:\n",
    "                doc_freq[word] = 1\n",
    "            else:\n",
    "                doc_freq[word] += 1\n",
    "\n",
    "    idf={}\n",
    "    for word, freq in doc_freq.items():\n",
    "        idf[word] = math.log(N/float(freq))  #idf\n",
    "    return idf\n",
    "\n",
    "IDF = computeIDF(docs=[docA, docB])\n",
    "print(IDF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da5fcff-8588-40da-ac84-b50788d46505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for doc1: {'I': 0.04780325383172036, 'am': 0.02390162691586018, 'Tulaja': 0.01195081345793009, 'Patil.': 0.01195081345793009, 'pursuing': 0.01195081345793009, 'Bachelors': 0.01195081345793009, 'of': 0.0, 'Computer': 0.01195081345793009, 'Engineering': 0.01195081345793009, 'in': 0.035852440373790276, 'GSM': 0.01195081345793009, 'College': 0.01195081345793009, 'Engineering.': 0.01195081345793009, 'It': 0.01195081345793009, 'is': 0.01195081345793009, 'one': 0.01195081345793009, 'the': 0.0, 'best': 0.01195081345793009, 'colleges': 0.01195081345793009, 'Pune,': 0.01195081345793009, 'falling': 0.01195081345793009, 'tenth': 0.01195081345793009, 'list.': 0.01195081345793009, 'have': 0.01195081345793009, 'good': 0.01195081345793009, 'knowledge': 0.01195081345793009, 'about': 0.01195081345793009, 'C,': 0.01195081345793009, 'C++,': 0.01195081345793009, 'Python,': 0.01195081345793009, 'Java,': 0.01195081345793009, 'Web': 0.01195081345793009, 'Development': 0.01195081345793009, 'and': 0.01195081345793009, 'Data': 0.01195081345793009, 'Analytics.': 0.01195081345793009, 'In': 0.01195081345793009, 'my': 0.01195081345793009, 'free': 0.01195081345793009, 'time,': 0.01195081345793009, 'like': 0.01195081345793009, 'listening': 0.01195081345793009, 'to': 0.01195081345793009, 'music': 0.01195081345793009, 'as': 0.01195081345793009, 'it': 0.01195081345793009, 'makes': 0.01195081345793009, 'me': 0.01195081345793009, 'relaxed.': 0.01195081345793009}\n",
      "TF-IDF for doc2: {'Peter': 0.08154672712469944, 'Piper': 0.08154672712469944, 'picked': 0.04077336356234972, 'a': 0.04077336356234972, 'peck': 0.08154672712469944, 'of': 0.0, 'pickled': 0.08154672712469944, 'peppers.': 0.02038668178117486, 'A': 0.02038668178117486, 'peppers': 0.04077336356234972, 'picked.': 0.02038668178117486, 'If': 0.02038668178117486, 'peppers,': 0.02038668178117486, 'Where’s': 0.02038668178117486, 'the': 0.0, 'picked?': 0.02038668178117486}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "def computeTFIDF(tf, idf):\n",
    "    tfidf={}\n",
    "    for word, tf_val in tf.items():\n",
    "        if word in idf:\n",
    "            tfidf[word] = tf_val * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidf1 = computeTFIDF(tf1, IDF)\n",
    "tfidf2 = computeTFIDF(tf2, IDF)\n",
    "\n",
    "print(\"TF-IDF for doc1:\", tfidf1)\n",
    "print(\"TF-IDF for doc2:\", tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea8350-697c-4588-b422-283001be1203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
